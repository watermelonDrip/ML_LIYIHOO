{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW02-1_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/watermelonDrip/ML_LIYIHOO/blob/master/HW02_1_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "* Slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW02/HW02.pdf\n",
        "* Video (Chinese): https://youtu.be/PdjXnQbu2zo\n",
        "* Video (English): https://youtu.be/ESRr-VCykBs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "90126870-8d8c-4d4e-b59a-f86d5f1b8016"
      },
      "source": [
        "!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR\n",
            "To: /content/data.zip\n",
            "100% 372M/372M [00:02<00:00, 183MB/s]\n",
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  timit_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs_AXgryJM1V",
        "outputId": "467714b8-5596-443a-b2c0-4bd7eeb912fe"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.zip  model.ckpt  prediction.csv  sample_data  timit_11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT3FC3wvJRyF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "7c9beb12-1200-4e2e-a97b-1b16a36d6dc4"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "\n",
        "print(train[1])\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n",
            "[-1.06031787e+00 -8.63627851e-01  1.22968948e+00  7.56692290e-02\n",
            "  1.48449290e+00  1.45324707e+00  1.57720077e+00  1.08029580e+00\n",
            "  1.22445810e+00  9.57690656e-01  6.89464390e-01  1.33886659e+00\n",
            "  5.69743216e-01  4.01845528e-03 -6.46719337e-02  2.17502117e-01\n",
            "  1.78295150e-02 -8.39905918e-01 -1.12149477e-01  1.06648453e-01\n",
            "  3.63196172e-02 -2.14937463e-01  1.75648510e-01  6.59720674e-02\n",
            "  1.56778574e-01  2.20319703e-01  7.37798680e-03 -2.85231531e-01\n",
            " -3.70392859e-01  1.88651308e-01 -6.14090145e-01  6.02679923e-02\n",
            " -1.31131560e-01  3.97200704e-01  2.25695267e-01  2.12702885e-01\n",
            "  1.56177590e-02  3.26819539e-01 -2.06116382e-02 -1.06123281e+00\n",
            " -9.03625906e-01  9.42397118e-01 -3.81120890e-02  1.65353727e+00\n",
            "  1.22543192e+00  1.17685366e+00  1.33255804e+00  1.31299901e+00\n",
            "  1.70436823e+00  7.87518263e-01  1.29268754e+00  3.90054166e-01\n",
            "  4.01581302e-02 -3.37548256e-01  2.88712144e-01  5.72003759e-02\n",
            " -1.20687795e+00  2.86794603e-02 -1.50001183e-01  4.05286580e-01\n",
            " -2.98659265e-01 -2.33184099e-01 -1.76584348e-01 -5.45328893e-02\n",
            "  2.21217513e-01  9.45613980e-02 -8.27450037e-01 -1.13819194e+00\n",
            "  3.90801787e-01 -6.28362969e-02  3.03181887e-01 -1.99815810e+00\n",
            "  4.66274470e-01  3.12472910e-01 -8.77222955e-01 -1.82582885e-01\n",
            " -2.33716592e-01 -4.03018534e-01 -1.06212807e+00 -9.08962548e-01\n",
            "  9.75875378e-01 -2.04413667e-01  1.22211385e+00  9.93508041e-01\n",
            "  8.76231849e-01  1.43398941e+00  1.26532602e+00  1.97007048e+00\n",
            "  8.72867465e-01  1.13416886e+00  9.52655673e-02  1.07997186e-01\n",
            " -7.28254497e-01 -9.91247073e-02  2.07148567e-02 -1.30949616e+00\n",
            " -4.99211997e-02 -1.00952947e+00  1.94749400e-01 -6.08527362e-02\n",
            " -1.24126054e-01  1.82864871e-02 -2.59372294e-01 -3.81689548e-01\n",
            "  2.12639257e-01 -8.58649135e-01 -8.64577234e-01 -1.14494510e-01\n",
            " -2.61136711e-01  2.57611321e-03 -1.90944779e+00 -3.64855826e-01\n",
            "  1.56460822e-01 -5.57713985e-01  6.92279488e-02 -5.68377793e-01\n",
            " -8.28798473e-01 -1.06191611e+00 -9.70546961e-01  9.24154639e-01\n",
            "  1.46834910e-01  9.20319498e-01  1.27266622e+00  1.16062248e+00\n",
            "  1.58957672e+00  1.30237186e+00  1.73544633e+00  6.06748164e-01\n",
            "  1.19970751e+00 -9.32869986e-02  1.95697382e-01 -1.02887332e+00\n",
            " -5.98904252e-01 -1.37208074e-01 -1.25656593e+00  2.71264520e-02\n",
            " -1.69064796e+00 -1.70910954e-01 -9.75871682e-02 -6.77761510e-02\n",
            "  5.19630373e-01 -6.05026260e-02 -5.24223149e-01  3.04244697e-01\n",
            " -8.74331117e-01 -2.71221966e-01 -4.47424710e-01 -5.76478779e-01\n",
            "  5.36577478e-02 -6.34124517e-01 -8.76622081e-01 -1.75224483e-01\n",
            " -6.45301938e-01  6.59369946e-01 -3.00959140e-01 -7.39374831e-02\n",
            " -1.05042028e+00 -1.06862378e+00  5.58611751e-01  1.10776991e-01\n",
            "  7.71914423e-01  1.23919809e+00  9.80679393e-01  1.36081409e+00\n",
            "  1.39392006e+00  1.31515992e+00  9.11246061e-01  1.64093864e+00\n",
            "  5.70503831e-01  3.28374028e-01 -1.24639738e+00 -5.96226037e-01\n",
            " -6.02521524e-02 -1.40948248e+00 -1.32102206e-01 -1.38518500e+00\n",
            " -4.29135561e-01 -6.18704140e-01 -5.11061788e-01  2.03825504e-01\n",
            " -3.05523515e-01 -4.12613809e-01  4.25035417e-01 -6.60828531e-01\n",
            "  1.34227246e-01 -3.75178345e-02 -8.73855799e-02 -8.24022889e-02\n",
            "  4.75858897e-01 -9.94346380e-01 -9.29976285e-01 -5.92114508e-01\n",
            " -5.11203669e-02 -2.72168573e-02  3.60568106e-01 -9.55076098e-01\n",
            " -1.58940804e+00  6.16136611e-01  1.71702251e-01  4.44531530e-01\n",
            "  1.52057469e+00 -4.31532651e-01  1.94033718e+00  1.16426265e+00\n",
            "  3.38921487e-01  2.47822583e-01  6.99799240e-01  9.47623327e-02\n",
            "  4.87677306e-01 -1.32175124e+00 -3.98572385e-01 -5.11770137e-02\n",
            " -1.61139297e+00 -4.61830273e-02 -9.16405618e-01 -6.23762488e-01\n",
            " -6.97170138e-01 -9.33167636e-01  3.21253061e-01 -9.09742936e-02\n",
            " -5.43052495e-01  4.98073280e-01 -1.17573440e-01  3.20442975e-01\n",
            " -1.50099874e-01  4.89678280e-03 -2.39009365e-01  1.23354352e+00\n",
            " -3.86144102e-01 -4.77330923e-01  7.68109113e-02  1.28474668e-01\n",
            "  3.91026318e-01 -9.63936597e-02 -8.40701520e-01 -1.81785882e+00\n",
            "  4.85490590e-01 -2.02655718e-01  4.21492569e-02  1.01531172e+00\n",
            " -7.92341292e-01  9.33648705e-01  1.37912941e+00  1.48887157e+00\n",
            "  1.05156589e+00  5.91378033e-01 -9.78660524e-01  6.59198225e-01\n",
            " -1.27271903e+00 -4.07840222e-01  9.65095460e-02 -1.42963982e+00\n",
            " -2.03768626e-01 -6.18606389e-01 -8.28281641e-01 -7.22793996e-01\n",
            " -6.49429500e-01  4.07665446e-02  6.35120124e-02 -5.95824838e-01\n",
            "  5.01842797e-01  5.02664506e-01  2.97489703e-01  2.53030598e-01\n",
            "  1.71916321e-01 -5.39968312e-01  1.48270547e+00 -1.09128229e-01\n",
            " -1.29797742e-01  8.80621135e-01 -4.71850932e-01  4.87448245e-01\n",
            " -2.49728322e-01 -7.30128646e-01 -2.06564474e+00  1.86883658e-01\n",
            " -3.65922928e-01 -9.96498689e-02  1.35353315e+00 -6.90742791e-01\n",
            "  6.87243164e-01  1.03387594e+00  1.27258444e+00  2.01744223e+00\n",
            "  1.35738683e+00  5.79804406e-02  7.70361304e-01 -1.05683970e+00\n",
            " -4.84485388e-01 -5.23381233e-02 -1.40884960e+00 -2.74756432e-01\n",
            " -1.50329188e-01 -9.15076315e-01 -6.52323902e-01 -4.12303478e-01\n",
            " -3.56568009e-01  7.45925829e-02 -4.82759655e-01  3.02662909e-01\n",
            "  9.80262697e-01 -7.49856308e-02  3.80662113e-01  1.39069304e-01\n",
            " -1.96973234e-02  1.94967616e+00  2.28254318e-01  3.60403627e-01\n",
            "  5.24278343e-01 -1.13539720e+00  4.35466647e-01  1.69133261e-01\n",
            " -5.20460784e-01 -2.21396041e+00  4.54778820e-01  1.37009159e-01\n",
            " -2.36613244e-01  9.44357753e-01  1.86723739e-01  7.26103544e-01\n",
            "  4.23629843e-02  5.93997777e-01  2.75454700e-01  8.02780032e-01\n",
            "  1.43263817e-01  7.54907012e-01 -6.56272054e-01 -3.51282090e-01\n",
            "  2.79432535e-01 -1.12508857e+00 -2.76002973e-01  3.25811177e-01\n",
            " -1.03030431e+00 -1.13547254e+00 -1.36951849e-01 -7.08771884e-01\n",
            "  5.15467077e-02 -7.88462102e-01 -1.14774883e-01  1.24326789e+00\n",
            " -1.17138013e-01  7.76109815e-01  7.18606234e-01  1.40111014e-01\n",
            "  1.05625045e+00  8.91476497e-02 -2.61040270e-01  1.09594189e-01\n",
            " -9.99418020e-01  1.70763090e-01  1.03426144e-01 -3.11334819e-01\n",
            " -2.19305611e+00  5.83996654e-01 -1.15972005e-01 -8.38180006e-01\n",
            "  1.16305590e+00  3.92338574e-01  9.73371744e-01  8.21318984e-01\n",
            "  5.63197315e-01  1.32885087e+00  1.42390704e+00 -8.61321390e-01\n",
            "  5.14843047e-01 -2.91104317e-02 -3.69456559e-01  5.72675049e-01\n",
            " -6.58208549e-01 -5.55097759e-01  9.18861151e-01 -9.11956489e-01\n",
            " -1.30992973e+00 -5.83235174e-03 -5.07001519e-01  3.79579306e-01\n",
            " -3.61937255e-01 -7.85685897e-01  1.24015629e+00 -2.78456789e-02\n",
            "  7.70842195e-01  1.13364911e+00 -2.09791645e-01 -1.97946578e-01\n",
            "  6.05132818e-01 -4.22132939e-01 -2.53685057e-01 -2.63690263e-01\n",
            " -4.16162431e-01  1.44490823e-01 -8.85201171e-02 -2.19145298e+00\n",
            "  2.29345962e-01  2.97451168e-01 -6.16865575e-01  7.37154424e-01\n",
            "  1.03101462e-01  6.08573437e-01  7.29585469e-01  1.34383321e+00\n",
            "  2.62638718e-01  1.33966327e+00 -7.79201448e-01  2.51110882e-01\n",
            "  7.82057464e-01 -2.96693623e-01  8.70073259e-01 -2.41779052e-02\n",
            " -2.00715557e-01  8.72241437e-01 -4.12164599e-01 -1.48735940e+00\n",
            " -6.36673272e-01 -1.77274323e+00 -1.10794693e-01 -4.83548552e-01\n",
            " -1.11266673e+00  1.87091041e+00  1.78902561e-03  2.79093772e-01\n",
            "  1.59681535e+00  1.73026130e-01 -7.31534302e-01  9.67214108e-02\n",
            " -2.98584163e-01 -1.29520237e-01 -8.25046837e-01 -1.04952157e+00\n",
            " -4.96883512e-01]\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYqi_lAuvC59",
        "outputId": "2934216e-f6c3-461d-f229-41f7276f8a37"
      },
      "source": [
        "VAL_RATIO = 0.2\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8rzkGraeYeN",
        "outputId": "4195527d-215a-4be3-ed2c-d22b84bff0cf"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.layer3 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "\n",
        "        self.act_fn = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTp3ZXg1yO9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f064e130-8a1d-4b21-9707-d83070dc19ee"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 20               # number of training epoch\n",
        "learning_rate = 0.0001       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdMWsBs7zzNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f22078-67ef-4ed8-b9fc-825b2e81d6da"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/020] Train Acc: 0.467302 Loss: 1.811661 | Val Acc: 0.567428 loss: 1.433065\n",
            "saving model with acc 0.567\n",
            "[002/020] Train Acc: 0.594383 Loss: 1.330666 | Val Acc: 0.628639 loss: 1.211098\n",
            "saving model with acc 0.629\n",
            "[003/020] Train Acc: 0.644506 Loss: 1.154064 | Val Acc: 0.660421 loss: 1.101216\n",
            "saving model with acc 0.660\n",
            "[004/020] Train Acc: 0.672215 Loss: 1.052246 | Val Acc: 0.676300 loss: 1.038718\n",
            "saving model with acc 0.676\n",
            "[005/020] Train Acc: 0.691347 Loss: 0.983104 | Val Acc: 0.685154 loss: 1.001852\n",
            "saving model with acc 0.685\n",
            "[006/020] Train Acc: 0.705615 Loss: 0.931955 | Val Acc: 0.689301 loss: 0.984177\n",
            "saving model with acc 0.689\n",
            "[007/020] Train Acc: 0.716344 Loss: 0.891687 | Val Acc: 0.694516 loss: 0.964627\n",
            "saving model with acc 0.695\n",
            "[008/020] Train Acc: 0.725881 Loss: 0.857907 | Val Acc: 0.697720 loss: 0.951889\n",
            "saving model with acc 0.698\n",
            "[009/020] Train Acc: 0.733718 Loss: 0.829495 | Val Acc: 0.696691 loss: 0.949866\n",
            "[010/020] Train Acc: 0.741153 Loss: 0.803701 | Val Acc: 0.699374 loss: 0.944832\n",
            "saving model with acc 0.699\n",
            "[011/020] Train Acc: 0.748050 Loss: 0.781106 | Val Acc: 0.697773 loss: 0.946494\n",
            "[012/020] Train Acc: 0.753793 Loss: 0.760380 | Val Acc: 0.702830 loss: 0.938236\n",
            "saving model with acc 0.703\n",
            "[013/020] Train Acc: 0.759404 Loss: 0.741234 | Val Acc: 0.700452 loss: 0.945627\n",
            "[014/020] Train Acc: 0.764573 Loss: 0.723574 | Val Acc: 0.702159 loss: 0.942118\n",
            "[015/020] Train Acc: 0.769470 Loss: 0.707325 | Val Acc: 0.704432 loss: 0.936154\n",
            "saving model with acc 0.704\n",
            "[016/020] Train Acc: 0.773687 Loss: 0.691314 | Val Acc: 0.701736 loss: 0.945714\n",
            "[017/020] Train Acc: 0.778677 Loss: 0.676633 | Val Acc: 0.701586 loss: 0.953081\n",
            "[018/020] Train Acc: 0.783107 Loss: 0.662425 | Val Acc: 0.699667 loss: 0.963290\n",
            "[019/020] Train Acc: 0.786396 Loss: 0.649180 | Val Acc: 0.700086 loss: 0.957681\n",
            "[020/020] Train Acc: 0.790645 Loss: 0.636623 | Val Acc: 0.699736 loss: 0.964269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKjtAScPWtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b919a025-62b0-46b2-8970-686ff2700826"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "0YRV7NvSIstP",
        "outputId": "97dfbf27-69f9-40b3-d3be-b78023cdcf33"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2b1118ebe439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_11' is not defined"
          ]
        }
      ]
    }
  ]
}